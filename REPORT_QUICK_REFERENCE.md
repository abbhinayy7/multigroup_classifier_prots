# ProteoBoostR Model Report - Quick Reference Card

**PRINT THIS PAGE** and keep it handy while completing your report!

---

## 1ï¸âƒ£ WHICH TEMPLATE TO USE?

| Your Model Has... | Use This Report |
|---|---|
| **2 classes** (e.g., Yes/No, Case/Control) | **BINARY_MODEL_REPORT_TEMPLATE.md** |
| **3+ classes** (e.g., Type A/B/C/D) | **MULTIGROUP_MODEL_REPORT_TEMPLATE.md** |

---

## 2ï¸âƒ£ KEY VALUES & WHERE TO FIND THEM

Copy these from your model output files:

### From `evaluation_results_[TS].tsv`:
```
[ACCURACY]          = Accuracy value Ã— 100 (add %)
[AUC_VALUE]         = AUC value (should be 0-1)
[THRESHOLD]         = Best_Threshold value
[SENSITIVITY]       = Sensitivity value Ã— 100 (add %)
[SPECIFICITY]       = Specificity value Ã— 100 (add %)
[PRECISION]         = Precision value Ã— 100 (add %)
```

### From `confusion_matrix_[TS].tsv`:
```
[TP] = True Positives (correctly predicted positive)
[TN] = True Negatives (correctly predicted negative)
[FP] = False Positives (incorrectly predicted positive)
[FN] = False Negatives (incorrectly predicted negative)

Quick check: TP + TN + FP + FN = total test samples
```

### From `train_matrix_[TS].tsv` & `test_matrix_[TS].tsv`:
```
[N_TRAIN]  = Row count of train_matrix (# training samples)
[N_TEST]   = Row count of test_matrix (# test samples)
[N_FEATURES] = Column count - 1 (subtract sample_id column)
```

### From `best_params_[TS].tsv`:
```
[eta]      = learning rate
[max_depth] = tree depth parameter
[subsample] = subsample parameter
[colsample_bytree] = feature sampling
[min_child_weight] = leaf sample minimum
[gamma]    = split gain threshold
[alpha]    = L1 regularization
[lambda]   = L2 regularization
```

### From filename:
```
[TIMESTAMP] = Model creation date (from filename)
[DATE]      = Today's date when writing report
```

---

## 3ï¸âƒ£ COMMON CALCULATIONS

**Calculate Standard Deviation (SD) from CV results:**
```
In Excel: =STDEV(range of fold accuracies)
Example: =STDEV(97.4%, 98.2%, 97.8%, 97.0%, 98.4%)
Result: Â±0.65%
```

**Verify Confusion Matrix:**
```
Sensitivity = TP / (TP + FN)    [Higher = better at catching positives]
Specificity = TN / (TN + FP)    [Higher = better at confirming negatives]
Precision = TP / (TP + FP)      [Higher = fewer false alarms]
Accuracy = (TP + TN) / Total    [Overall correctness]
```

**Check if Balanced:**
```
Class 0 count: [N]
Class 1 count: [N]
Ratio: [larger] / [smaller]
Assessment:
  - <1.5:1 â†’ Balanced âœ“
  - 1.5-3:1 â†’ Slightly imbalanced
  - >3:1 â†’ Highly imbalanced
```

---

## 4ï¸âƒ£ SECTION CHECKLIST

Fill these sections in this order:

```
â”Œâ”€ HEADER & EXECUTIVE SUMMARY
â”‚  â”œâ”€ [DATE]              â†’ Today's date
â”‚  â”œâ”€ [DATASET NAME]      â†’ Your dataset name
â”‚  â”œâ”€ [MODEL TIMESTAMP]   â†’ From filename
â”‚  â”œâ”€ [ACCURACY]%         â†’ evaluation_results.tsv
â”‚  â”œâ”€ [AUC_VALUE]         â†’ evaluation_results.tsv
â”‚  â””â”€ [THRESHOLD]         â†’ evaluation_results.tsv
â”‚
â”œâ”€ BACKGROUND SECTION
â”‚  â”œâ”€ [POSITIVE CLASS]    â†’ Your domain knowledge
â”‚  â”œâ”€ [NEGATIVE CLASS]    â†’ Your domain knowledge
â”‚  â””â”€ [USE CASES]         â†’ Why you built this model
â”‚
â”œâ”€ METHODS SECTION
â”‚  â”œâ”€ [N_SAMPLES]         â†’ Annotation file
â”‚  â”œâ”€ [N_FEATURES]        â†’ Protein matrix columns - 1
â”‚  â”œâ”€ [N_TRAIN]           â†’ train_matrix row count
â”‚  â”œâ”€ [N_TEST]            â†’ test_matrix row count
â”‚  â””â”€ Hyperparameters     â†’ best_params.tsv
â”‚
â”œâ”€ RESULTS SECTION
â”‚  â”œâ”€ [TP], [TN], [FP], [FN]  â†’ confusion_matrix.tsv
â”‚  â”œâ”€ All percentages         â†’ evaluation_results.tsv
â”‚  â”œâ”€ ROC image               â†’ roc_curve_[TS].png
â”‚  â””â”€ Feature importance      â†’ top proteins list
â”‚
â”œâ”€ QUALITY SECTION
â”‚  â”œâ”€ Train vs Test accuracy  â†’ Compare CV vs test
â”‚  â”œâ”€ CV results              â†’ From training log
â”‚  â””â”€ Threshold analysis      â†’ evaluation_results
â”‚
â””â”€ CONTEXT (Your Knowledge!)
   â”œâ”€ Why this classification matters
   â”œâ”€ What the top proteins do
   â”œâ”€ Biological interpretation
   â”œâ”€ Limitations
   â””â”€ Next steps
```

---

## 5ï¸âƒ£ QUICK INTERPRETATION GUIDE

### **Is My Accuracy Good?**
```
< 70%  â†’ Poor (worse than many simple methods)
70-80% â†’ Acceptable (reasonable discrimination)
80-90% â†’ Good (strong separation)
> 90%  â†’ Excellent (very predictive)
```

### **Is My AUC Good?**
```
< 0.60 â†’ Poor discrimination
0.60-0.70 â†’ Fair
0.70-0.80 â†’ Good
0.80-0.90 â†’ Very good
> 0.90 â†’ Excellent
1.00 â†’ Perfect (caution: possible overfitting)
```

### **Is There Overfitting?**
```
Train Acc - Test Acc = Difference

< 5%   â†’ Good generalization âœ“
5-10%  â†’ Slight overfitting (âš  acceptable)
> 10%  â†’ Significant overfitting (âš  concerning)
```

### **Sensitivity vs Specificity Trade-off**
```
High Sensitivity (>95%):
  âœ“ Catches most positives (good for screening)
  âœ— More false positives (more follow-up tests)

High Specificity (>95%):
  âœ“ Fewer false alarms (confident predictions)
  âœ— Misses some true positives (might miss cases)

Balanced (80-90%):
  âœ“ Good overall performance
  â†’ Best for most clinical use
```

---

## 6ï¸âƒ£ RED FLAGS âš ï¸

Stop and review if you see:

```
â˜ Accuracy = 99% but dataset is only 20 samples
  â†’ Likely overfitting, results may not generalize

â˜ AUC = 0.5 (random guessing)
  â†’ Model not working, check data/parameters

â˜ Sensitivity = 100%, Specificity = 100%
  â†’ Likely data contamination or leakage

â˜ Feature importance very different from biology
  â†’ Unexpected proteins, verify interpretation

â˜ Test accuracy much worse than CV accuracy
  â†’ Overfitting or different data distribution

â˜ Sample numbers don't add up
  â†’ Check: N_TRAIN + N_TEST = expected total

â˜ [BRACKETED] placeholders still visible
  â†’ You missed filling something in!
```

---

## 7ï¸âƒ£ BEFORE YOU SHARE - VALIDATION CHECKLIST

```
Content Check:
â˜ All [FILLED] with real values (no brackets left)
â˜ Numbers consistent across sections
â˜ Date field updated to today
â˜ Dataset/model names consistent
â˜ TP + TN + FP + FN = N_TEST samples

Math Check:
â˜ Sensitivity = TP/(TP+FN), should equal report value
â˜ Specificity = TN/(TN+FP), should equal report value
â˜ Accuracy = (TP+TN)/Total, should equal report value
â˜ Probabilities between 0-1
â˜ Percentages between 0-100%

Format Check:
â˜ No spelling errors
â˜ Headings are consistent
â˜ Tables aligned properly
â˜ Images/plots visible
â˜ File saved with clear name

Biology Check:
â˜ Results make biological sense
â˜ Top proteins are plausible
â˜ Conclusions not overclaimed
â˜ Limitations acknowledged
â˜ Methods reproducible
```

---

## 8ï¸âƒ£ TOP MISTAKES TO AVOID

| Mistake | Wrong | Right |
|---|---|---|
| **Confusing formats** | 0.9818 vs 98.18% | Specify which! Use 98.18% for percentages |
| **Wrong metric** | Using train accuracy as final | Always use test accuracy |
| **Missing reference** | "98% accuracy" | "98.18% accuracy on 17 test samples" |
| **Unit confusion** | "[THRESHOLD]" = 0.7614 | "[THRESHOLD]" = 0.7614 (already decimal, no %) |
| **Forgetting sample size** | "AUC = 0.95" (sounds great!) | "AUC = 0.95 on N=20 samples" (different perception) |
| **Math errors** | TP=7, FN=0, Sensitivity="100%" | âœ“ Correct: 7/(7+0)=100% |
| **Unfilled placeholders** | Report has "[VALUE]" in it | Replace ALL [BRACKETS] before sharing |
| **Wrong template** | Binary model using multigroup template | Match template to number of classes |

---

## 9ï¸âƒ£ FILE NAMING EXAMPLES

**GOOD Examples:**
```
âœ“ GBM_Binary_Model_Report_Feb2026.md
âœ“ Proteomics_Aggressive_vs_Standard_Report.md
âœ“ Project_Multigroup_Subtype_Classification_v1.md
âœ“ LUAD_4Class_Model_Report_2026-02-10.md
```

**AVOID:**
```
âœ— report.md (too vague)
âœ— [TEMPLATE].md (sounds like unfinished)
âœ— model (missing extension)
âœ— final_final_v3_realdone.md (unprofessional)
```

---

## ğŸ”Ÿ AFTER COMPLETION

```
Step 1: Save as .md file
  â†’ File > Save As > [ProjectName]_Report.md

Step 2: (Optional) Convert to PDF
  â†’ Use pandoc: pandoc report.md -o report.pdf
  â†’ Or: Online converter (pandoc.org/try)

Step 3: Share with stakeholders
  â†’ Email as attachment
  â†’ Share in Git/Sharepoint
  â†’ Print for meetings

Step 4: Collect feedback
  â†’ Ask: Can you understand the results?
  â†’ Ask: Do you want different sections?
  â†’ Ask: Are there questions about predictions?

Step 5: Update next time
  â†’ Keep template for next model
  â†’ Note what worked/didn't work
  â†’ Refine sections based on feedback
```

---

## BONUS: ONE-PAGE SUMMARY FOR BUSY PEOPLE

If someone asks "Can you summarize your model in one page?":

**Copy this template:**

```markdown
# [Model Name] - 1-Page Summary

**What:** XGBoost classifier distinguishing [CLASS_A] from [CLASS_B]
**Data:** [N_TRAIN] training samples, [N_TEST] test samples
**Features:** [N_FEATURES] protein measurements

**Results:**
- Accuracy: [VALUE]% 
- AUC: [VALUE]
- Sensitivity: [VALUE]% (catches [CLASS_A])
- Specificity: [VALUE]% (confirms [CLASS_B])

**Top 5 Proteins:**
1. [PROTEIN]: [INTERPRETATION]
2. [PROTEIN]: [INTERPRETATION]
3. [PROTEIN]: [INTERPRETATION]
4. [PROTEIN]: [INTERPRETATION]
5. [PROTEIN]: [INTERPRETATION]

**Decision Rule:**
If probability > [THRESHOLD], predict [CLASS_A]
Otherwise predict [CLASS_B]

**Use Case:** [WHY THIS MATTERS]
**Limitations:** [KEY CAVEATS]
```

**Print & paste - Done!**

---

## EMERGENCY REFERENCE

**"What number should go here?"**

```
Location on Page    â†’ Check This File              â†’ What It Contains
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Accuracy/AUC        â†’ evaluation_results_[TS].tsv â†’ Top metrics
Sample size         â†’ train_matrix_[TS].tsv       â†’ Row count
Feature count       â†’ best_params_[TS].tsv        â†’ Protein count
Confusion matrix    â†’ confusion_matrix_[TS].tsv   â†’ TP/TN/FP/FN
Cross-validation    â†’ proteoboostr_[TS].log       â†’ CV accuracy per fold
Date                â†’ System clock / your calendar â†’ Today's date
Hyperparameters     â†’ best_params_[TS].tsv        â†’ All 8 parameters
Probability values  â†’ predicted_prob_[TS].tsv     â†’ Per-sample scores
```

---

## FINAL CHECKLIST

```
Before you hit "send", ask yourself:

â–¡ Can a non-scientist understand what problem this solves?
â–¡ Can a clinician understand the decision rule?
â–¡ Can a researcher reproduce this analysis?
â–¡ Are the limitations clearly stated?
â–¡ Is the data quality discussed?
â–¡ Are caveats mentioned (not just strengths)?
â–¡ Did I proofread for typos?
â–¡ Are all images included and visible?
â–¡ Is my conclusion honest (not overclaimed)?
â–¡ Would I stake my reputation on these results?

If YES to all â†’ Ready to share! ğŸ‰
If NO to any â†’ Fix before sharing âœï¸
```

---

**Reference Sheet Version:** 1.0  
**Created:** February 10, 2026  
**For:** ProteoBoostR Model Reporting System

**Keep this card handy! Print it and tape to your desk while writing reports.**
